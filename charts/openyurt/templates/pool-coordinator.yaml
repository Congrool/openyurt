apiVersion: v1
kind: Service
metadata:
  name: pool-coordinator-apiserver
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    openyurt.io/topologyKeys: openyurt.io/nodepool
  labels:
    name: pool-coordinator
spec:
  type: ClusterIP
  ports:
    - port: 443
      targetPort: {{ .Values.poolCoordinator.apiserverSecurePort }}
      protocol: TCP
      name: https
  selector:
    k8s-app: pool-coordinator
---
apiVersion: v1
kind: Service
metadata:
  name: pool-coordinator-etcd
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    openyurt.io/topologyKeys: openyurt.io/nodepool
  labels:
    name: pool-coordinator
spec:
  type: ClusterIP
  ports:
    - port: 2379
      targetPort: {{ .Values.poolCoordinator.etcdPort }}
      protocol: TCP
      name: https
  selector:
    k8s-app: pool-coordinator
---
apiVersion: apps.openyurt.io/v1alpha1
kind: YurtAppDaemon
metadata:
  name: pool-coordinator
  namespace: {{ .Release.Namespace | quote }}
spec:
  selector:
    matchLabels:
      k8s-app: pool-coordinator
  nodepoolSelector:
    matchLabels:
      openyurt.io/node-pool-type: "edge"
  workloadTemplate:
    deploymentTemplate:
      metadata:
        labels:
          k8s-app: pool-coordinator
      spec:
        replicas: 1
        selector:
          matchLabels:
            k8s-app: pool-coordinator
        template:
          metadata:
            labels:
              k8s-app: pool-coordinator
          spec:
        initContainers:
          - image: alpine:3.14
            name: init-tools
            command:
              - /bin/sh
              - -c
              - |
                sed -i "s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g" /etc/apk/repositories              
                apk add busybox-static curl
                cp $(which busybox.static) /tmp/tools/busybox
                ARCH=$(uname -m)
                VERSION={{ .Values.poolCoordinator.apiserverImage.tag }}
                case $ARCH in
                  x86_64)
                    URL="https://dl.k8s.io/release/$VERSION/bin/linux/amd64/kubectl"
                    ;;
                  armv6l|armv7l)
                    URL="https://dl.k8s.io/release/$VERSION/bin/linux/arm/kubectl"
                    ;;
                  aarch64)
                    URL="https://dl.k8s.io/release/$VERSION/bin/linux/arm64/kubectl"
                    ;;
                  *)
                    echo "Unsupported architecture: $ARCH"
                    exit 1
                    ;;
                esac
                echo "Downloading kubectl from $URL"
                curl -LO $URL
                chmod +x kubectl
                mv ./kubectl /tmp/tools/kubectl
            volumeMounts:
              - mountPath: /tmp/tools
                name: rbac-inject-tools            
            containers:
              - command:
                - /tmp/tools/busybox
                - sh
                - -c
                - |
                  KUBECTL="/tmp/tools/kubectl"
                  SLEEP="/tmp/tools/busybox sleep"
                  (
                    ${SLEEP} 5
                    while true; do
                      echo "Trying to inject rbac..."
                      ${KUBECTL} apply -f /tmp/rbac --kubeconfig /etc/kubernetes/pki/admin.conf
                      if [ $? -eq 0 ]; then
                        break
                      fi
                      ${SLEEP} 1
                    done
                    echo "Successfully injected rbac!"
                  ) &

                  exec kube-apiserver \
                  --bind-address=0.0.0.0 \
                  --allow-privileged=true \
                  --anonymous-auth=true \
                  --authorization-mode=Node,RBAC \
                  --client-ca-file=/etc/kubernetes/pki/ca.crt \
                  --enable-admission-plugins=NodeRestriction \
                  --enable-bootstrap-token-auth=true \
                  --disable-admission-plugins=ServiceAccount \
                  --etcd-cafile=/etc/kubernetes/pki/ca.crt \
                  --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt \
                  --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key \
                  --etcd-servers=https://127.0.0.1:{{ .Values.poolCoordinator.etcdPort }} \
                  --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt \
                  --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key \
                  --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname \
                  --secure-port={{ .Values.poolCoordinator.apiserverSecurePort }} \
                  --service-account-issuer=https://kubernetes.default.svc.cluster.local \
                  --service-account-key-file=/etc/kubernetes/pki/sa.pub \
                  --service-account-signing-key-file=/etc/kubernetes/pki/sa.key \
                  --service-cluster-ip-range={{ .Values.poolCoordinator.serviceClusterIPRange }} \
                  --tls-cert-file=/etc/kubernetes/pki/apiserver.crt \
                  --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
                image: "{{ .Values.poolCoordinator.apiserverImage.registry }}/{{ .Values.poolCoordinator.apiserverImage.repository }}:{{ .Values.poolCoordinator.apiserverImage.tag }}"
                imagePullPolicy: {{ .Values.poolCoordinator.apiserverImage.pullPolicy }}
                livenessProbe:
                  failureThreshold: 8
                  httpGet:
                    host: 127.0.0.1
                    path: /livez
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                name: kube-apiserver
                readinessProbe:
                  failureThreshold: 3
                  httpGet:
                    host: 127.0.0.1
                    path: /readyz
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  periodSeconds: 1
                  successThreshold: 1
                  timeoutSeconds: 15
                {{- if .Values.poolCoordinator.apiserverResources }}
                resources:
                  {{- toYaml .Values.poolCoordinator.apiserverResources | nindent 18 }}
                {{- end }}
                startupProbe:
                  failureThreshold: 24
                  httpGet:
                    host: 127.0.0.1
                    path: /livez
                    port: {{ .Values.poolCoordinator.apiserverSecurePort }}
                    scheme: HTTPS
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                terminationMessagePath: /dev/termination-log
                terminationMessagePolicy: File
                volumeMounts:
                  - mountPath: /etc/kubernetes/pki
                    name: pool-coordinator-certs
                    readOnly: true
                  - mountPath: /tmp/tools
                    name: rbac-inject-tools
                  - mountPath: /tmp/rbac
                    name: rbac-injected                  
              - command:
                  - etcd
                  - --advertise-client-urls=https://0.0.0.0:{{ .Values.poolCoordinator.etcdPort }}
                  - --listen-client-urls=https://0.0.0.0:{{ .Values.poolCoordinator.etcdPort }}
                  - --cert-file=/etc/kubernetes/pki/etcd-server.crt
                  - --client-cert-auth=true
                  - --max-txn-ops=102400
                  - --data-dir=/var/lib/etcd
                  - --max-request-bytes=100000000
                  - --key-file=/etc/kubernetes/pki/etcd-server.key
                  - --listen-metrics-urls=http://0.0.0.0:{{ .Values.poolCoordinator.etcdMetricPort }}
                  - --snapshot-count=10000
                  - --trusted-ca-file=/etc/kubernetes/pki/ca.crt
                image: "{{ .Values.poolCoordinator.etcdImage.registry }}/{{ .Values.poolCoordinator.etcdImage.repository }}:{{ .Values.poolCoordinator.etcdImage.tag }}"
                imagePullPolicy: {{ .Values.imagePullPolicy }}
                name: etcd
                {{- if .Values.poolCoordinator.etcdResources}}
                resources:
                  {{- toYaml .Values.poolCoordinator.etcdResources | nindent 18 }}
                {{- end }}
                startupProbe:
                  failureThreshold: 24
                  httpGet:
                    host: 127.0.0.1
                    path: /health
                    port: {{ .Values.poolCoordinator.etcdMetricPort }}
                    scheme: HTTP
                  initialDelaySeconds: 10
                  periodSeconds: 10
                  successThreshold: 1
                  timeoutSeconds: 15
                volumeMounts:
                  - mountPath: /var/lib/etcd
                    name: etcd-data
                  - mountPath: /etc/kubernetes/pki
                    name: pool-coordinator-certs
                    readOnly: true
            dnsPolicy: ClusterFirst
            {{- if .Values.imagePullSecrets }}
            imagePullSecrets:
            {{ toYaml .Values.imagePullSecrets | nindent 14 }}
            {{- end }}
            enableServiceLinks: true
            hostNetwork: true
            preemptionPolicy: PreemptLowerPriority
            priority: 2000001000
            priorityClassName: system-node-critical
            restartPolicy: Always
            schedulerName: default-scheduler
            securityContext:
              seccompProfile:
                type: RuntimeDefault
            terminationGracePeriodSeconds: 30
            volumes:
              - emptyDir:
                  medium: Memory
                name: etcd-data
              - emptyDir:
                name: rbac-inject-tools
              - projected:
                  defaultMode: 420
                  sources:
                    - secret:
                        name: pool-coordinator-dynamic-certs
                    - secret:
                        name: pool-coordinator-static-certs
                name: pool-coordinator-certs
              - projected:
                  defaultMode: 420
                  sources:
                    - configMap:
                        name: pool-coordinator-rbac-injected
                name: rbac-injected
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: openyurt:pool-coordinator:node-lease-proxy-client 
rules:
  - apiGroups:
      - "coordination.k8s.io"
    resources:
      - leases
    verbs:
      - get
      - update
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: openyurt:pool-coordinator:node-lease-proxy-client
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: openyurt:pool-coordinator:node-lease-proxy-client
subjects:
  - apiGroup: rbac.authorization.k8s.io
    kind: User
    name: openyurt:pool-coordinator:node-lease-proxy-client
---
apiVersion: v1
kind: ConfigMap 
metadata:
  name: pool-coordinator-rbac-injected
  namespace: kube-system
data:
  pool-coordinator-apiserver.yaml: |
    apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: openyurt:pool-coordinator:apiserver
    subjects:
      - kind: User 
        apiGroup: rbac.authorization.k8s.io
        name: openyurt:pool-coordinator:apiserver
    roleRef:
      apiGroup: rbac.authorization.k8s.io
      kind: ClusterRole
      name: system:kubelet-api-admin